<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Lab 9 - Fast Robots</title>
    <link rel="stylesheet" href="styles.css">
</head>

<body>
    <script src="scripts.js"></script>
    <script type="text/javascript" async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <div class="container">
        <aside class="sidebar">
            <div class="profile">
                <h1>Fast Robots</h1>
                <h3>Spring 2025</h3>
            </div>
            <nav>
                <ul>
                    <li>
                        <h3>Lab 9 - Mapping</h3>
                    </li>
                    <li>
                        <a href="index.html">
                            <h4>Home</h4>
                        </a>
                    </li>

                    <li>
                        <a href="#orientation">
                            <h4>Orientation Control</h4>
                        </a>
                    </li>

                    <li>
                        <a href="#readings">
                            <h4>Readings</h4>
                        </a>
                    </li>

                    <li>
                        <a href="#transformation">
                            <h4>Transformation</h4>
                        </a>
                    </li>

                    <li>
                        <a href="#edges">
                            <h4>Edges</h4>
                        </a>
                    </li>

                    <li>
                        <a href="#conclusion">
                            <h4>Notes and Conclusion</h4>
                        </a>
                    </li>

                </ul>
            </nav>
        </aside>

        <main class="content">
            <section class="lab-content">
                <div class="general-text">

                    <h2>Lab 9: Stunts!</h2>

                    <p><i>04.08.2025 - 04.15.2025</i></p>

                    <p>In this lab, we started building a static map of the lab environment using distance data from the
                        robot’s ToF sensors. I used PID control on orientation to make the robot perform on-axis
                        rotations at multiple marked positions, collecting evenly spaced distance readings as it turned.
                        After gathering all the data, I transformed the measurements into the room’s reference frame and
                        plotted them to create a map. This map will be used in future labs for localization and
                        navigation.</p>

                    <section id="orientation">
                        <h3>Orientation Control</h3>

                        <p>For the mapping routine, I decided to use orientation control since my Lab 6 PID had already
                            shown solid and consistent performance. I chose to rotate the robot in 24-degree increments,
                            capturing 15 distinct orientations at each scanning location, and took 5 distance and IMU
                            measurements at each angle to improve reliability. To support this process, I added a
                            START_MAPPING BLE command to the Artemis to initiate the orientation-based scanning routine.
                        </p>

                        <div class="code-snippet">
                            <pre>
case START_MAP:
{
    // setup
    set_up_tof();
    pid_ori_i = 0;
    increment = 24;
    num_readings = 5;
    start_time = (float)millis();

    // initialize control variables
    pid_ori_error_threshold = 5;
    error_ori = 0;
    prev_error_ori = 0;
    error_sum_ori = 0;
    current_angle = 0;

    distanceSensor1.startRanging();
    distanceSensor2.startRanging();

    for (int i = -180; i < 180; i += increment)
    {
        target_angle = i;
        do
        {
            pid_ori(target_angle);
            delay(10);
        } while (!(abs(error_ori) < pid_ori_error_threshold));

        drive(0, 0);
        delay(10);

        int j = 0;

        while (j < num_readings)
        {
            if (distanceSensor1.checkForDataReady() && distanceSensor2.checkForDataReady() && myICM.dataReady() && pid_ori_i < num_data_msgs)
            {
                float yaw_g = 0, roll_g = 0, pitch_g = 0, dt = 0;
                unsigned long last_time = millis();
                if (j != 0)
                {
                    last_time = times[pid_ori_i - 1];
                }
                const float alpha = 0.0735;

                times[pid_ori_i] = millis();

                distanceSensor1.startRanging();
                distance1_data[pid_ori_i] = distanceSensor1.getDistance();
                distanceSensor1.clearInterrupt();
                distanceSensor1.stopRanging();

                distanceSensor2.startRanging();
                distance2_data[pid_ori_i] = distanceSensor2.getDistance();
                distanceSensor2.clearInterrupt();
                distanceSensor2.stopRanging();

                // yaw
                myICM.getAGMT();

                unsigned long current_tm = millis();

                dt = (current_tm - last_time) / 1000.;
                last_time = current_tm;
                yaw_g = yaw_g + myICM.gyrZ() * dt;
                gyro_yaw_raw[pid_ori_i] = yaw_g;

                pid_ori_i++;
                j++;
            }
            delay(100);
        }
    }

    for (int i = 0; i < pid_ori_i; i++)
    {
        tx_estring_value.clear();
        tx_estring_value.append(times[i]);
        tx_estring_value.append(",");
        tx_estring_value.append(distance1_data[i]);
        tx_estring_value.append(",");
        tx_estring_value.append(distance2_data[i]);
        tx_estring_value.append(",");
        tx_estring_value.append(gyro_yaw_raw[i]);
        tx_characteristic_string.writeValue(tx_estring_value.c_str());
    }
    break;
}
</pre>
                        </div>

                        <p>I had to tweak my PID code and parameters from Lab 6 to be able to handle smaller changes in
                            angle. I iterated through 360 degrees by using a for loop. At each angle, the robot pauses
                            to take 5 distance readings using the ToF sensors. These readings are stored and averaged to
                            improve accuracy. The current orientation angle is recorded using IMU data for precise
                            angular tracking. Once the scan at a position is complete, the robot continues on to the
                            next angle. The mapping data is formatted into a structured message, including distance and
                            orientation, to simplify post-processing.
                        </p>
                        <p>One issue I had was not having enough power to my wheels for small angle changes. Tweaking my
                            parameters helped, but ultimately I had to clip any values smaller than 75 to 75. This
                            ensured that even with small errors, the car would still be able to overcome the ground's
                            friction and rotate.</p>


                    </section>

                    <section id="readings">
                        <h3>Readings</h3>

                        <p>After implementing the mapping command, I had to take data at four different points in order
                            to map the robots surroundings. I wasn't able to finish in time to do the mapping in lab, so
                            I took measurements in a hallway.</p>

                        <img src="images/lab9_images/hallway.JPG" alt="pi_angle" class="lab-photo" width="400px">

                        <p>The videos below show mapping in action at (0, 12), (-12,0), and (-24,0), respectively.</p>

                        <iframe width="750" height="450" src="https://youtube.com/embed/KZrs3Tgd5q0"
                            title="YouTube video player" frameborder="0"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

                        <iframe width="750" height="450" src="https://youtube.com/embed/Q6Ef_db28gI"
                            title="YouTube video player" frameborder="0"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

                        <iframe width="750" height="450" src="https://youtube.com/embed/Tmlt0vdk3d4"
                            title="YouTube video player" frameborder="0"
                            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
                            referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

                        <p>After collecting data for each angle, I sent over the distance and yaw values via BLE. I
                            plotted the average distances for each location on polar axes to ensure that the data I was
                            collecting was valid.</p>

                        <img src="images/lab9_images/tof_p1.png" alt="pi_angle" class="lab-photo" height="400px"
                            width="400px">
                        <img src="images/lab9_images/tof_p2.png" alt="pi_angle" class="lab-photo" height="400px"
                            width="400px">
                        <img src="images/lab9_images/tof_p3.png" alt="pi_angle" class="lab-photo" height="400px"
                            width="400px">
                        <img src="images/lab9_images/tof_p4.png" alt="pi_angle" class="lab-photo" height="400px"
                            width="400px">

                        <p>From these graphs, I could tell that the second sensor was shifted by 90 degrees, as the
                            sensors are placed in the front and on the side of the car.</p>

                        <p>While testing, I noticed that the robot was pretty inconsistent with what it considered to be
                            0 degrees. Therefore, I concluded that I was better off trusting the orientation from the
                            gyroscope. Furthermore, the error threshold I had was 5 degrees, so there will always be
                            some kind of error that makes readings not equally spaced.</p>

                    </section>

                    <section id="transformation">
                        <h3>Transformation</h3>
                        <p>
                            Next, I converted the distance measurements taken relative to the robot at each scan
                            location into the arena's global reference coordinates (<i>x</i> and <i>y</i>). I did
                            this by defining a three-dimensional transformation matrix <b>T</b>, such that:
                        </p>
                        <pre>
                        T = [ R   d ] = [ cos(θ) -sin(θ)  x ]
                            [ 0   1 ]   [ sin(θ)  cos(θ)  y ]
                                        [   0       0     1 ]
                            </pre>
                        <p>
                            I also defined two position vectors <b>P₁</b> and <b>P₂</b> for the two ToF sensors
                            relative to the center of the robot:
                        </p>
                        <pre>
                        P₁ = [ TOF₁ ]
                             [   0   ]
                             [   1   ]
                        
                        P₂ = [ -TOF₂ ]
                             [   0    ]
                             [   1    ]
                            </pre>
                        <p>
                            In my case, the front sensor (TOF₁) is mounted about 2.5 inches in front of the center
                            of the robot, while the side sensor (TOF₂) is located about 1 inches to the left of
                            center. I applied these offsets to the distances measured by each sensor during mapping
                            and transformed the resulting points into the environment's global reference frame.
                        </p>
                        <p>
                            When all the data is plotted together, the result is a spatial map of the environment
                            from four distinct scanning locations. Because the second sensor was giving me smaller
                            values that seemed off, I decided to just use my ToF 1 data.
                        </p>

                        <img src="images/lab9_images/tof_map.png" alt="pi_angle" class="lab-photo" width="600px">

                        <p>When I first generated the map, I noticed that a lot of the points weren't quite aligned in a
                            hallway-looking shape. I realized that because my robot didn't start at the same
                            orientation, I would have to rotate some of the points to make it the correct orientation.
                        </p>

                    </section>

                    <section id="edges">
                        <h3>Edges</h3>
                        <p>
                            Finally, using the map from prior, I drew lines according to the trend of distance points.
                        </p>

                        <img src="images/lab9_images/tof_map_lines.png" alt="pi_angle" class="lab-photo" width="600px">

                        <p>Comparing it to the hallway, I would say that it matched up pretty well. One slight mistake
                            was that my distance measurements at (0,0) were actually closer to the right wall than
                            intended. Thus, the orange points show that there is a wall directly to its right, when it
                            reality that wall was more likely to be where the red points' right wall are.</p>

                        <p>From the lines that I drew, I made rough estimates for the starting and ending coordinates
                            for each of these lines</p>

                        <table border="1" style="border-collapse: collapse; width: 100%; text-align: center;">
                            <thead>
                                <tr>
                                    <th>Line Number</th>
                                    <th>Starting Coordinates (x, y)</th>
                                    <th>Ending Coordinates (x, y)</th>
                                </tr>
                            </thead>
                            <tbody>
                                <tr>
                                    <td>1</td>
                                    <td>(-33, -3)</td>
                                    <td>(-28, -3)</td>
                                </tr>
                                <tr>
                                    <td>2</td>
                                    <td>(-28, -3)</td>
                                    <td>(-28, -7)</td>
                                </tr>
                                <tr>
                                    <td>3</td>
                                    <td>(-28, -7)</td>
                                    <td>(-17, -7)</td>
                                </tr>
                                <tr>
                                    <td>4</td>
                                    <td>(-17, -7)</td>
                                    <td>(-17, -3)</td>
                                </tr>
                                <tr>
                                    <td>5</td>
                                    <td>(-17, -3)</td>
                                    <td>(-5, -3)</td>
                                </tr>
                                <tr>
                                    <td>6</td>
                                    <td>(-5, -3)</td>
                                    <td>(-5, -9)</td>
                                </tr>
                                <tr>
                                    <td>7</td>
                                    <td>(4, -9)</td>
                                    <td>(4, 23)</td>
                                </tr>
                                <tr>
                                    <td>8</td>
                                    <td>(-4, 23)</td>
                                    <td>(-4, 4)</td>
                                </tr>
                                <tr>
                                    <td>9</td>
                                    <td>(-4, 4)</td>
                                    <td>(-33, 4)</td>
                                </tr>
                            </tbody>
                        </table>

                    </section>

                    <section id="conclusion">
                        <h3>Notes and Conclusion</h3>

                        <p>I worked with Jennie Redrovan, Sana Chawla, Daniela Tran, and Henry Calderon on this lab. I
                            also referenced Stephen Wagner's Lab 7 Report from last year.</p>
                    </section>

                </div>

            </section>
        </main>
    </div>
</body>

</html>